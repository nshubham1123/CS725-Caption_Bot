{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":["import os\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import string\n","#import cv2\n","from PIL import Image\n","from keras.preprocessing import image\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Embedding, Input, LSTM\n","from tensorflow.keras.layers import Add\n","from tensorflow.keras.applications.inception_v3 import InceptionV3\n","from tensorflow.keras.applications.inception_v3 import preprocess_input\n","from tensorflow.keras.models import Model\n","import nltk\n","from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n","import matplotlib.pyplot as plt"],"execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["%matplotlib inline"],"execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## generating tokens \n","mapping image name to list of captions"]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":["filename = \"Flickr_Data/Flickr_TextData/Flickr8k.token.txt\"\n","file = open(filename,\"r\")\n","doc = file.readlines()"],"execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["descriptions ={} # a dictionary where key is image_id and value is list of 5 captions \n","for line in doc:\n","    tokens = line.split(\"\\t\")\n","    img_id = tokens[0].split(\".\")[0]\n","    img_desc = tokens[1].strip('\\n').lower() #converting all characters into lower case\n","    if img_id not in descriptions:\n","        descriptions[img_id]=[]\n","    descriptions[img_id].append(img_desc)\n","descriptions"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ntically in black shirts and black and red plants as they climb on a fence .',\n","  'two children in identical outfits hang on a chain link fence .',\n","  'two girls wearing red pants , black pants , and sandals are climbing a chain link fence .',\n","  'two identically dressed girls climbing a fence',\n","  'two young girls in matching outfits standing on a chain link fence .'],\n"," '2081679622_6f1442367d': ['a boy in grey pajamas is jumping on the couch .',\n","  'a boy in pajama pants jumps on a red couch .',\n","  'a boy smiles at the camera and jumps in the air over the couch .',\n","  'a shirtless boy jumps onto the couch .',\n","  'the boy in pajama pants jumps off the sofa .'],\n"," '2082005167_a0d6a70020': ['a black and brown dog is playing on the ice at the edge of a lake .',\n","  'a black dog is breaking ice at the edge of a stream .',\n","  'a black dog is stepping on the ice .',\n","  'a black dog plays with a bit of ice by the frozen river .',\n","  'a dog stands on the edge of an icy body of water .'],\n"," '2083434441_a93bc6306b': ['an elderly woman is riding a bicycle in the city as a yellow taxi is about to pass by .',\n","  'an elderly woman rides a bicycle along a city street .',\n","  'an older woman with blond hair rides a bicycle down the street .',\n","  'a woman in a grey overcoat rides her bicycle along a street .',\n","  'older woman wearing glassses riding a bicycle with a shopping bag on the handle , yellow car is in the background .'],\n"," '2083778090_3aecaa11cc': ['a man in the snow wearing a furry hat and a black jacket with a red scarf',\n","  'a man wearing a hat in a snowstorm .',\n","  'a man with a goatee wears a fur hat in the snow .',\n","  'man in furry hat stands in snow',\n","  'the man is wearing a scarf , jacket , and a furry hat on a snowy day .'],\n"," '2084103826_ffd76b1e3e': ['a black and white dog is jumping in the grass .',\n","  'a black and white dog is running with a muzzle .',\n","  'a black and white dog wearing a face mask runs through a field .',\n","  'a black and white dog with a mask on its face is running in a field .',\n","  'the black and white muzzled dog is leaping above the grass .'],\n"," '2084157130_f288e492e4': ['a black and white dog runs beside a brown dog in a green field .',\n","  'the black and white dog along with the brown dog are galloping outside',\n","  'two dogs race through a field .',\n","  'two dogs run across the grassy field .',\n","  'two dogs running fast in the grass .'],\n"," '2084217208_7bd9bc85e5': ['a person in a blue jacket , wearing a bicycle helmet is riding a bike',\n","  'a woman is riding her bicycle .',\n","  'a woman riding a bike in a park',\n","  'a woman with a blue jacket wears a helmet as she rides a bike .',\n","  'a woman with a helmet riding a bike .'],\n"," '2085078076_b9db242d21': ['a small boy standing near muddy water with something in his hand while someone beside him washes clothing in buckets .',\n","  'a young black child stands on the edge of a body of water near buckets .',\n","  'a young boy stands near the water in a muddy river .',\n","  'child standing by a rivers edge with laundry in buckets next to them .',\n","  'on shore near brown water , boy holds object beside buckets of rags .'],\n"," '2085255128_61224cc47f': ['a boy high in the air above the dirt and water near the red rock cliffs .',\n","  'a medium sized child jumps off of a dusty bank over a creek .',\n","  'a young person wearing a grey shirt and yellow shorts jumping into a river .',\n","  'person in yellow shorts , grey shirt with logo jumping from bank into the water .',\n","  'the boy in yellow shorts leaps from river bank towards the river .'],\n"," '2085400856_ae09df33a7': ['a dog in the snow .',\n","  'a dog laying in the snow .',\n","  'a dog lays on the snow .',\n","  'a golden dog plays in the snow',\n","  'a red dog crouches in the snow .'],\n"," '2085403342_a17b0654fe': ['a child in a bright red jacket runs through a pile of hay .',\n","  'a little girl in a red sweater and blue skirt is in the hay .',\n","  'a young child dances on a straw surface .',\n","  'the boy is running through hay at a pumpkin patch .',\n","  'the child is walking through straw .'],\n"," '2085557551_7a88d01d4e': ['a boy holding kitchen utensils and making a threatening face',\n","  'a boy is pulling a wild face whilst waving cooking implements in his hands .',\n","  'a girl in a tan shirt holding up a fork and knife in one hand and a pot in the other',\n","  'a person with cooking utilities poses menacingly in a dark environment .',\n","  'a woman with a knife and fork grimmaces .'],\n"," '2085726719_a57a75dbe5': ['two girls bundled up in winter coats pose for a picture .',\n","  'two girls holding drinks and looking at something on a cellphone .',\n","  'two teen girls are looking at a small electronic device while wearing winter coats .',\n","  'two women dressed for cold weather in jackets and gloves look at something on a cellphone screen .',\n","  'two young girls huddle to look at a cellphone .'],\n"," '2086513494_dbbcb583e7': ['a girl in a white coat takes pictures .',\n","  'a girl in a white puffy jacket takes a picture .',\n","  'a group of people looking into their cameras',\n","  'asian woman in white coat takes photo .',\n","  'the woman in the white coat is holding a camera in her hand .'],\n"," '2086532897_b8714f2237': ['a kayaker falling down a waterfall',\n","  'a man is kayaking down a waterfall .',\n","  'a man on a kayak coming down a raging waterfall',\n","  'a person is on a small boat and going down some very strong water currents .',\n","  'white water rafter goes through a rough spot .'],\n"," '2086534745_1e4ab80078': ['a man in a yellow boat paddling down rough waters .',\n","  'a rafter in a yellow raft in a steamy white river .',\n","  'a yellow kayak is descending quickly down a white water rapid .',\n","  'kayacker riding rough current downstream .',\n","  'the yellow kayak is in the middle of the rapids .'],\n"," '2086678529_b3301c2d71': ['a girl in a purple dress smiling as she climbs an indoor rock wall .',\n","  'a girl is in a dress .',\n","  'a little girl wearing a purple dress looks down at the camera .',\n","  'a young girl climbs a rock wall , in a purple dress .',\n","  'a young girl wears a purple dress .'],\n"," '2087317114_cf06df5aa5': ['a mani is kayaking down some calmer rapids .',\n","  'a man in a small kayak is floating down the river .',\n","  'a man in a yellow jacket is floating on the river in a green kayak .',\n","  'a man smiles in his blue and green kayak .',\n","  'a man wearing a helmet in a pyranha kayak floats down some river rapids .'],\n"," '2087640654_1a84577a44': ['a blonde dog is digging a deep hole in the ground and its tail is up in the air .',\n","  'a dog half way in a hole in the ground .',\n","  'a dog is digging a large hole in a yard to find something .',\n","  'a dog is digging in the ground and half of his body is under the ground .',\n","  'the rear end of an animal whose front end is underground .'],\n"," '2088120475_d6318364f5': ['a person dressed up as a snowman is walking through a crowd of people .',\n","  'five cold people in the snowstorm , one wearing a snowman head',\n","  'people pose for a picture in the snow',\n","  'three young people pose by a snowman in a red tie .',\n","  'two couples pose with a snowman .'],\n"," '2088460083_42ee8a595a': ['a broken down hummer gets towed on a truck bed .',\n","  'a damaged vehicle is carried by a repair truck in a night-time scene .',\n","  'a flat bed truck in a parking lot with an army vehicle on its bed .',\n","  'an old , beat-up jeep being towed away .',\n","  'dirty suv sits on the bed of a tow truck at night .'],\n"," '2088532947_c628e44c4a': ['the girl in the white shirt has a bracelet on and has her arm around the girl in the peach shirt .',\n","  'the two girls laid on the grass together .',\n","  'two little girls are laying on the grass smiling .',\n","  'two young girls holding each other on the grassy ground',\n","  'two young girls hug each other in the grass .'],\n"," '2088910854_c6f8d4f5f9': ['a large man is wearing a headband with mistletoe attached to it .',\n","  'a man in a sweatshirt and cap wears a flower on his head .',\n","  'a man wearing a black knit cap with a red and white striped headband with a flower on top .',\n","  'an man with a black ski cap and a red and white headband looks into the distance .',\n","  'man wearing a black hat with a christmas theme .'],\n"," '2089122314_40d5739aef': ['a dog running with a newspaper in its mouth .',\n","  'a dog runs down the street , carrying a newspaper .',\n","  'a dog with a newspaper in his mouth runs down a paved path .',\n","  'a fluffy , tri-colored dog is running down the road with a paper in its mouth .',\n","  'an australian shepherd dog carrying a newspaper in his mouth .'],\n"," '2089350172_dc2cf9fcf6': ['three black dogs are swimming through some murky brown water , one with a yellow object in its mouth .',\n","  'three black dogs swimming in dirty water',\n","  'three dark dogs swimming and one of them has a ball .',\n","  'three dogs are up to their necks in water .',\n","  'three dogs swim in the water .'],\n"," '2089426086_7acc98a3a8': ['a girl in a hooded jacket presents a cake with candles .',\n","  'a girl in a hood jacket holds a birthday cake with many candles .',\n","  'a girl in a winter coat with a birthday cake .',\n","  'a girl is holding a birthday cake .',\n","  'a young girl holding a cake with candles dressed in a heavy coat .'],\n"," '2089442007_6fc798548c': ['a brown dog is walking in the water .',\n","  'a brown dog takes cautious steps in the calm waters with a sandy bottom .',\n","  'a brown dog wades in the water .',\n","  'a dog with red fur stands in a body of water .',\n","  'a large , golden colored dog is walking through water .'],\n"," '2089539651_9e518ec7de': ['a hiker with two hiking poles stands on a hillside with a view of the city below him .',\n","  'a hiker with two sticks is a standing on a path that is high up on a hill .',\n","  'a hiking man stands on a cliff overlooking a city .',\n","  'a man hikes on a dirt path high above a city .',\n","  'a person holding poles is standing on a rock ledge with bushes on each side overlooking mountains .'],\n"," '2089542487_b4c1ee7025': ['a hiker is ready to take on a rugged outdoor trail .',\n","  'a lone hiker on a dirt hiking trail with a hat and sunglasses on with trees in the background',\n","  'a man in shorts is hiking in a dry region .',\n","  'a man stops for a picture with the scenery during his hike .',\n","  'the man is posing for a picture in the desert wearing a green shirt and hat , and holding a metal detector .'],\n"," '2089555297_95cf001fa7': ['a man in a hat with long sleeves and long pants stands overlooking a city .',\n","  'a man stands on a cliff overlooking a city .',\n","  'a man with a backpack and hat is standing by a high cliff where you can see for miles .',\n","  'hiker standing on top of a rock overlooking a town .',\n","  'the man wearing a backpack and a hat , stands on a ledge overlooking a city .'],\n"," '2090327868_9f99e2740d': ['a woman , a little boy , and a tiny baby are standing in front of a cow statue .',\n","  'a woman and two small children posing in front of a large cow statue .',\n","  'a woman gives a baby to a small child to hold as they stand in front of a cow statue .',\n","  'a woman stands with two children near a decorated bull statue .',\n","  'the woman and children are standing next to a statue of a cow .'],\n"," '2090339522_d30d2436f9': ['a man in a green hat is someplace up high .',\n","  'a man in a green hat is taking a self portrait .',\n","  'a man in a hat at a scenic overlook .',\n","  'a man wearing a green canvas hat with a distnat city in the background',\n","  'man with green hate and sunglasses on a high up peak'],\n"," '2090386465_b6ebb7df2c': ['a woman and man in winter gear stand .',\n","  'three adults wearing cold weather gear .',\n","  'two people are wearing heavy clothing .',\n","  'two people in winter clothing .',\n","  'two women bundled up on a snowy day'],\n"," '2090387793_274ab4cf7d': ['a boy wearing black walks down the street with his hands in fists .',\n","  'a teenage boy dressed in black stands on a snowy street .',\n","  'a teenage boy wearing all black tries to look tough with cheap jewelry on .',\n","  'a young man wearing a black hat and jacket is walking outside .',\n","  'punk rock teenager walks with a mean look on his face .'],\n"," '2090545563_a4e66ec76b': ['the boy laying face down on a skateboard is being pushed along the ground by another boy .',\n","  'two girls play on a skateboard in a courtyard .',\n","  'two people play on a long skateboard .',\n","  'two small children in red shirts playing on a skateboard .',\n","  'two young children on a skateboard going across a sidewalk'],\n"," '2090723611_318031cfa5': ['dogs walking in the grassy field .',\n","  'large fluffy grey dog standing in grass in front of a brown dog .',\n","  'one dog standing in some grass with his tongue hanging out and a tan dog in the background',\n","  'two dogs of different breeds are standing in a pasture .',\n","  'two dogs stand in the grass .'],\n"," '2090997177_76d482b158': ['a black dog bounds across the sand .',\n","  'a dog walking across the desert .',\n","  'a short black dog is walking over a barren landscape .',\n","  'a small black dog running across the sand .',\n","  'a small black dog walks in the sand .'],\n"," '2091171488_c8512fec76': ['a female wearing pink gloves and a brown jacket is smiling in the snow .',\n","  'a woman standing in the snow with people in the background .',\n","  'a young woman in a spring coat and purple gloves stands in falling snow .',\n","  'a young woman looks happy in winter clothes while it snows .',\n","  'young asian woman with pink gloves stands in the snow .'],\n"," '2092177624_13ab757e8b': ['a boy in african clothing runs across a concrete wall .',\n","  'a little boy with a green shirt is running on a rock ledge .',\n","  'an african boy in large green shirt is running barefoot along a wall .',\n","  'a young boy dressed in green running along a smooth , stone wall near a wooded area .',\n","  'boy in green tunic on rock wall .'],\n"," '2092419948_eea8001d0f': ['a long haired man walking away beside a canal whist holding onto a railing .',\n","  'a man walks down a path next to a waterway .',\n","  'a person walks by some blue-green water while holding a rail .',\n","  'a person with a long ponytail walks along a waterway under a tunnel',\n","  'the man with long hair is walking along a storm drain .'],\n"," '2092870249_90e3f1855b': ['a man lies on a street with his head and shoulders under a parked car that is jacked up .',\n","  'a man looks under a car on a wet street .',\n","  'a man works on a car .',\n","  'a person lies half under a car , face-down .',\n","  'a person underneath a car with a jack .'],\n"," '2094323311_27d58b1513': [\"a dog places his head on a man 's face .\",\n","  'a dog snuggles with a man in sunglasses .',\n","  \"a dog with a blue collar is licking a man who 's wearing large tinted sunglasses .\",\n","  \"a large brown dog with a blue collar is resting his chin on a man 's face .\",\n","  'an orange dog is licking a man wearing red sunglasses .'],\n"," '2094543127_46d2f1fedf': [\"a girl holding a crocodile 's mouth closed\",\n","  'a girl is posed over a statue of a dangerous animal .',\n","  'a girl sits on what appears to be a crocodile .',\n","  'a girl sitting on top of a crocodile',\n","  'a girl with red hair is sitting on a crocodile holding its jaw .'],\n"," '2094810449_f8df9dcdf7': ['a boy stands in the ocean lifting up his shorts .',\n","  'a child in blue shorts is standing ankle deep in the water .',\n","  'a little boy in blue shorts , glasses , and a coat stands in the surf .',\n","  'the kid is wearing a blue jacket and standing in shallow beach water .',\n","  'the small child wades ankle deep in the water on the shoreline .'],\n"," '2095007523_591f255708': ['a baby looks through his crib .',\n","  'a baby , surrounded by some toys , looks out of a playpen .',\n","  'baby in playpen with wide eyes',\n","  'baby looking through playpen bars at camera .',\n","  'the infant is laying on its stomach , in a crib , looking up at the camera .'],\n"," '2095078658_c14ba89bc2': ['a dog runs through snow in front of the woods .',\n","  'large dog running in snow',\n","  'the dog runs through the snow .',\n","  'the german shepherd dog is running through the snow .',\n","  'the german shepherd is running through the white snow .'],\n"," '2095435987_1b7591d214': ['a dog lying on its back in the sand .',\n","  'a white and brown dog rolls on its back in the sand .',\n","  'the dog rolls over to scratch its back in the sand .',\n","  'the white and brown dog is rolling in the sand .',\n","  'the white dog is on its back in the sand with paws in the air .'],\n"," '2095444126_201ff9f222': ['a dog is shaking off water .',\n","  'a dog shakes his head .',\n","  'a dog shakes off water',\n","  'a dog shakes off water and gets contorted .',\n","  'the dog is shaking water off of his body .'],\n"," '2095478050_736c4d2d28': ['a man and a boy play music as little girls watch .',\n","  'a man is playing a guitar with a child playing a harmonica while three young girls watch .',\n","  'three children watch a man onstage playing guitar .',\n","  'three little girls look on as a boy plays the harmonica with an adult guiutarist sitting beside him .',\n","  'three little girls watch a band .'],\n"," '209605542_ca9cc52e7b': ['a climber wearing a red headband is pulling himself up some grey rocks high above some green foliage .',\n","  'a man in a headband climbing a rock .',\n","  'a man with a red headband climbing a rock cliff looming over greenery .',\n","  'man climbing a sheet rock face .',\n","  'man in red headband climbing a rock'],\n"," '2096771662_984441d20d': ['a man looks through his binoculars while another man holds a drink .',\n","  'a man with a thermos is standing next to a man who is gazing through binoculars .',\n","  'two men are standing together while one looks through binoculars .',\n","  'two men look out as one is holding binoculars .',\n","  'two people standing next to each other with mountains in the distance .'],\n"," '2097398349_ff178b3f1b': ['a hiker waves to the camera as he standing in front of snowcapped mountains .',\n","  'a man stands on the snow near the mountains .',\n","  'a person poses in the snow .',\n","  'a person stands in the snow at the top of a mountian , arms raised .',\n","  'a skier in the mountains .'],\n"," '2097403787_77a154f5b9': ['a lone person standing on some rocks with large snowy mountains in the background .',\n","  'a man is standing on the rocky ground in front of the mountain .',\n","  'a man standing in front of an icy mountain .',\n","  'the person is standing by the water in front of the snow topped mountains .',\n","  'the traveller pauses in front of the mountain view .'],\n"," '2097407245_c798e0dcaf': ['a man in a winter jacket and knit cap stand with his arms up at his side in a snow landscape .',\n","  'a man is standing on dirt with his hands in the air with mountains and snow behind him .',\n","  'a man raises his arms on rocky beach .',\n","  'one lone man with fist in the air with large rocky mountains in the background',\n","  'man with outstretched arms standing on snow-covered rocks .'],\n"," '2097420505_439f63c863': ['a male hiker wearing a green jacket is posing next to a large glacier .',\n","  'a man with a camera and an ice structure behind him .',\n","  'a man with a camera crouching on ice .',\n","  'a man with a camera kneels in front of a giant hill of snow .',\n","  'the mountain climber prepares to take a photo .'],\n"," '2097489021_ca1b9f5c3b': ['a girl in a ballet fairy costume is touched by a wand .',\n","  'a young girl performing the nutcracker .',\n","  'the little girl is standing on a stage dressed up as a pink fairy .',\n","  'the tiny girl dressed in a butterfly ballet costume , waves her wand near a nutcracker statue .',\n","  'young girl waving turquiose leotard and pink tutu with wings waving wand .'],\n"," '2098174172_e57d86ea03': ['a hiker is posing in front of snowy mountains .',\n","  'a hiker is standing in before the mountain holding two walking sticks .',\n","  'a man stands in front of snow covered mountains .',\n","  'a man with two ski poles stands near a mountaintop .',\n","  'the man wears a green hat and jacket while standing in front of a mountain .'],\n"," '2098418613_85a0c9afea': ['a brown chow mix dog is standing in front of a van with euro plats in the middle of a messy yard .',\n","  'a brown dog standing in a muddy yard .',\n","  'a brown dogs walks near a green van and some junk .',\n","  'a large brown dog stands in front of a green van in the yard of a house .',\n","  'a yellow dog is standing in front of a green car and next to a toilet .'],\n"," ...}"]},"metadata":{},"execution_count":6}]},{"metadata":{},"cell_type":"markdown","source":["## Data cleaning\n","1. removing punctuations\n","2. removing single characters\n","3. converting to lower case"]},{"metadata":{"trusted":true},"cell_type":"code","source":["# prepare translation table for removing punctuation\n","table = str.maketrans('', '', string.punctuation)\n","\n","for img_id, captions in descriptions.items():\n","    for i in range(len(captions)):\n","        desc = captions[i].split()\n","        # remove punctuation from each token\n","        desc = [w.translate(table) for w in desc]\n","        desc = [word for word in desc if len(word)>1 and word.isalpha()]\n","        \n","        captions[i] = \" \".join(desc)\n"],"execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## Preparing Train captions"]},{"metadata":{"trusted":true},"cell_type":"code","source":["# <============ Prepating list of image ids of train data ===============>\n","def load_data(filename):\n","    file = open(filename,'r')\n","    doc = file.readlines()\n","    dataset = []\n","    image_folder_path = 'Flickr_Data/Images/'\n","    \n","    for line in doc:\n","        if len(line)<1:\n","            continue\n","        image_id = line.split('.')[0]\n","        image_path = image_folder_path + image_id + '.jpg'\n","        if(os.path.isfile(image_path)):\n","            dataset.append(image_id)\n","    return dataset\n"," \n","# <================== mapping dataset images to their captions ============>\n","'''\n","parameter 1: 'all captions' is 'descriptions' dictionary which we created earlier\n","parameter 2 : dataset: train, dev or test\n","'''\n","def map_captions(all_captions, dataset): \n","    dataset_desc={}\n","    for img_id in dataset:\n","        captions = all_captions[img_id]\n","        for i in range(len(captions)):\n","            if img_id not in dataset_desc:\n","                dataset_desc[img_id]=[]\n","            desc =captions[i].split()\n","            desc = 'startseq ' + ' '.join(desc) + ' endseq'\n","            dataset_desc[img_id].append(desc)\n","    return dataset_desc\n","\n","#train_descriptions\n","train_images_file_name = \"Flickr_Data/Flickr_TextData/Flickr_8k.trainImages.txt\"\n","train_dataset = load_data(train_images_file_name)  \n","train_descriptions = map_captions(descriptions, train_dataset)\n","\n","# <================== keeping words with word count >=10 =============>\n","train_captions=[]\n","\n","for key, values in train_descriptions.items():\n","    for val in values:\n","        train_captions.append(val)\n","        \n","\n","# <============ getting frequency for all words ======================>\n","word_counts={}\n","for cap in train_captions:\n","    for w in cap.split(' '):\n","        word_counts[w] = word_counts.get(w, 0) + 1\n","        \n","\n"],"execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# <============ filtering words with freq>=10 ======================>\n","vocab = [w for w in word_counts if word_counts[w] >= 10]\n","\n","# <=========== index to word and word to index mapping===============>  \n","w_to_ix={}\n","ix_to_w={}\n","i=0\n","for word in vocab:\n","    w_to_ix[word] = i\n","    ix_to_w[i]=word\n","    i+=1\n","print(len(w_to_ix))\n","print(len(vocab))"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["1651\n1651\n"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":["# description with most number of words\n","max_length = max(len(d.split()) for d in train_captions)\n","max_length"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["34"]},"metadata":{},"execution_count":10}]},{"metadata":{},"cell_type":"markdown","source":["## InceptionV3 model"]},{"metadata":{"trusted":true,"tags":["outputPrepend"]},"cell_type":"code","source":["# Load the inception v3 model\n","model = InceptionV3(weights='imagenet')\n","# removing the last layer (output layer) from the inception v3\n","model_CNN = Model(model.input, model.layers[-2].output)\n","model_CNN.summary()"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["d_6[0][0]        \n__________________________________________________________________________________________________\nbatch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]                  \n__________________________________________________________________________________________________\nactivation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]     \n__________________________________________________________________________________________________\nactivation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]     \n__________________________________________________________________________________________________\nactivation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]     \n__________________________________________________________________________________________________\nactivation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]     \n__________________________________________________________________________________________________\nmixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_60[0][0]              \n                                                                 activation_63[0][0]              \n                                                                 activation_68[0][0]              \n                                                                 activation_69[0][0]              \n__________________________________________________________________________________________________\nconv2d_72 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_72 (BatchNo (None, 17, 17, 192)  576         conv2d_72[0][0]                  \n__________________________________________________________________________________________________\nactivation_72 (Activation)      (None, 17, 17, 192)  0           batch_normalization_72[0][0]     \n__________________________________________________________________________________________________\nconv2d_73 (Conv2D)              (None, 17, 17, 192)  258048      activation_72[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]                  \n__________________________________________________________________________________________________\nactivation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]     \n__________________________________________________________________________________________________\nconv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]                     \n__________________________________________________________________________________________________\nconv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]                  \n__________________________________________________________________________________________________\nactivation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]     \n__________________________________________________________________________________________________\nactivation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]     \n__________________________________________________________________________________________________\nconv2d_71 (Conv2D)              (None, 8, 8, 320)    552960      activation_70[0][0]              \n__________________________________________________________________________________________________\nconv2d_75 (Conv2D)              (None, 8, 8, 192)    331776      activation_74[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_71 (BatchNo (None, 8, 8, 320)    960         conv2d_71[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_75 (BatchNo (None, 8, 8, 192)    576         conv2d_75[0][0]                  \n__________________________________________________________________________________________________\nactivation_71 (Activation)      (None, 8, 8, 320)    0           batch_normalization_71[0][0]     \n__________________________________________________________________________________________________\nactivation_75 (Activation)      (None, 8, 8, 192)    0           batch_normalization_75[0][0]     \n__________________________________________________________________________________________________\nmax_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]                     \n__________________________________________________________________________________________________\nmixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_71[0][0]              \n                                                                 activation_75[0][0]              \n                                                                 max_pooling2d_3[0][0]            \n__________________________________________________________________________________________________\nconv2d_80 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_80 (BatchNo (None, 8, 8, 448)    1344        conv2d_80[0][0]                  \n__________________________________________________________________________________________________\nactivation_80 (Activation)      (None, 8, 8, 448)    0           batch_normalization_80[0][0]     \n__________________________________________________________________________________________________\nconv2d_77 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]                     \n__________________________________________________________________________________________________\nconv2d_81 (Conv2D)              (None, 8, 8, 384)    1548288     activation_80[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_77 (BatchNo (None, 8, 8, 384)    1152        conv2d_77[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_81 (BatchNo (None, 8, 8, 384)    1152        conv2d_81[0][0]                  \n__________________________________________________________________________________________________\nactivation_77 (Activation)      (None, 8, 8, 384)    0           batch_normalization_77[0][0]     \n__________________________________________________________________________________________________\nactivation_81 (Activation)      (None, 8, 8, 384)    0           batch_normalization_81[0][0]     \n__________________________________________________________________________________________________\nconv2d_78 (Conv2D)              (None, 8, 8, 384)    442368      activation_77[0][0]              \n__________________________________________________________________________________________________\nconv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_77[0][0]              \n__________________________________________________________________________________________________\nconv2d_82 (Conv2D)              (None, 8, 8, 384)    442368      activation_81[0][0]              \n__________________________________________________________________________________________________\nconv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_81[0][0]              \n__________________________________________________________________________________________________\naverage_pooling2d_7 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]                     \n__________________________________________________________________________________________________\nconv2d_76 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]                  \n__________________________________________________________________________________________________\nconv2d_84 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_7[0][0]        \n__________________________________________________________________________________________________\nbatch_normalization_76 (BatchNo (None, 8, 8, 320)    960         conv2d_76[0][0]                  \n__________________________________________________________________________________________________\nactivation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]     \n__________________________________________________________________________________________________\nactivation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]     \n__________________________________________________________________________________________________\nactivation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]     \n__________________________________________________________________________________________________\nactivation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_84 (BatchNo (None, 8, 8, 192)    576         conv2d_84[0][0]                  \n__________________________________________________________________________________________________\nactivation_76 (Activation)      (None, 8, 8, 320)    0           batch_normalization_76[0][0]     \n__________________________________________________________________________________________________\nmixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_78[0][0]              \n                                                                 activation_79[0][0]              \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 8, 8, 768)    0           activation_82[0][0]              \n                                                                 activation_83[0][0]              \n__________________________________________________________________________________________________\nactivation_84 (Activation)      (None, 8, 8, 192)    0           batch_normalization_84[0][0]     \n__________________________________________________________________________________________________\nmixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_76[0][0]              \n                                                                 mixed9_0[0][0]                   \n                                                                 concatenate[0][0]                \n                                                                 activation_84[0][0]              \n__________________________________________________________________________________________________\nconv2d_89 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_89 (BatchNo (None, 8, 8, 448)    1344        conv2d_89[0][0]                  \n__________________________________________________________________________________________________\nactivation_89 (Activation)      (None, 8, 8, 448)    0           batch_normalization_89[0][0]     \n__________________________________________________________________________________________________\nconv2d_86 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]                     \n__________________________________________________________________________________________________\nconv2d_90 (Conv2D)              (None, 8, 8, 384)    1548288     activation_89[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_86 (BatchNo (None, 8, 8, 384)    1152        conv2d_86[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_90 (BatchNo (None, 8, 8, 384)    1152        conv2d_90[0][0]                  \n__________________________________________________________________________________________________\nactivation_86 (Activation)      (None, 8, 8, 384)    0           batch_normalization_86[0][0]     \n__________________________________________________________________________________________________\nactivation_90 (Activation)      (None, 8, 8, 384)    0           batch_normalization_90[0][0]     \n__________________________________________________________________________________________________\nconv2d_87 (Conv2D)              (None, 8, 8, 384)    442368      activation_86[0][0]              \n__________________________________________________________________________________________________\nconv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_86[0][0]              \n__________________________________________________________________________________________________\nconv2d_91 (Conv2D)              (None, 8, 8, 384)    442368      activation_90[0][0]              \n__________________________________________________________________________________________________\nconv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_90[0][0]              \n__________________________________________________________________________________________________\naverage_pooling2d_8 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]                     \n__________________________________________________________________________________________________\nconv2d_85 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]                  \n__________________________________________________________________________________________________\nconv2d_93 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_8[0][0]        \n__________________________________________________________________________________________________\nbatch_normalization_85 (BatchNo (None, 8, 8, 320)    960         conv2d_85[0][0]                  \n__________________________________________________________________________________________________\nactivation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]     \n__________________________________________________________________________________________________\nactivation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]     \n__________________________________________________________________________________________________\nactivation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]     \n__________________________________________________________________________________________________\nactivation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_93 (BatchNo (None, 8, 8, 192)    576         conv2d_93[0][0]                  \n__________________________________________________________________________________________________\nactivation_85 (Activation)      (None, 8, 8, 320)    0           batch_normalization_85[0][0]     \n__________________________________________________________________________________________________\nmixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_87[0][0]              \n                                                                 activation_88[0][0]              \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_91[0][0]              \n                                                                 activation_92[0][0]              \n__________________________________________________________________________________________________\nactivation_93 (Activation)      (None, 8, 8, 192)    0           batch_normalization_93[0][0]     \n__________________________________________________________________________________________________\nmixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_85[0][0]              \n                                                                 mixed9_1[0][0]                   \n                                                                 concatenate_1[0][0]              \n                                                                 activation_93[0][0]              \n__________________________________________________________________________________________________\navg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]                    \n==================================================================================================\nTotal params: 21,802,784\nTrainable params: 21,768,352\nNon-trainable params: 34,432\n__________________________________________________________________________________________________\n"]}]},{"metadata":{},"cell_type":"markdown","source":["## get image feature vector\n","input is id of image"]},{"metadata":{"trusted":true},"cell_type":"code","source":["def get_image_feature_vector(image_id):\n","    # path of image\n","    image_folder_path = 'Flickr_Data/Images/'\n","    image_path = image_folder_path + image_id + '.jpg'\n","    # converting image to required size of inception model\n","    #print('image id is ',image_id)\n","    img = image.load_img(image_path, target_size=(299, 299))\n","    # Convert image to numpy array of 3-dimensions\n","    img_arr = image.img_to_array(img)\n","    #rescaling image\n","    imag_arr = img_arr/255.0\n","    # Add one more dimension\n","    img_arr = np.expand_dims(img_arr, axis=0)\n","    #print('image size is' , img_arr.shape)\n","    \n","    # preprocess the images using preprocess_input() from inception module\n","    img_arr = preprocess_input(img_arr)\n","    \n","    image_feature_vector = model_CNN.predict(img_arr) # Get the encoding vector for the image\n","    image_feature_vector = np.reshape(image_feature_vector, image_feature_vector.shape[1]) # reshape from (1, 2048) to (2048, )\n","    return image_feature_vector\n","    "],"execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## Data Generator"]},{"metadata":{"trusted":true},"cell_type":"code","source":["'''\n","Parameter 1: Dict with key as image_id and value as list of captions associated with image\n","Parameter 2: word-to-index conversion dict\n","Parameter 3: vocabulary\n","Parameter 4: max_length of a sentences\n","Parameter 5: batch size of data generated\n","'''\n","# data generator, intended to be used in a call to model.fit_generator()\n","def data_generator(train_descriptions, w_to_ix, vocab, max_length, batch_size):\n","    image_vectors, caption_sequences, output_sequences = [], [], []\n","    n=0\n","    # loop for ever over images\n","    while 1:\n","        for image_id, captions_list in train_descriptions.items():\n","            n+=1\n","            # retrieve the image vector\n","            image_feature_vector = get_image_feature_vector(image_id)\n","            \n","            for caption in captions_list:\n","                # encode the sequence\n","                caption_sequence = [w_to_ix[word] for word in caption.split(' ') if word in vocab]\n","                # split one sequence into multiple X, y pairs\n","                for i in range(1, len(caption_sequence)):\n","                    # split into input and output pair\n","                    input_sequence, output_sequence = caption_sequence[:i], caption_sequence[i]\n","                    # pad input sequence\n","                    input_sequence = pad_sequences([input_sequence], maxlen=max_length)[0]\n","                    # encode output sequence\n","                    output_sequence = to_categorical([output_sequence], num_classes=len(vocab))[0]\n","                    \n","                    image_vectors.append(image_feature_vector)\n","                    caption_sequences.append(input_sequence)\n","                    output_sequences.append(output_sequence)\n","                    \n","            # yield the batch data\n","            if n==batch_size:\n","                #print(np.array(image_vectors).shape,np.array(caption_sequences).shape,np.array(output_sequences).shape)\n","                yield ([np.array(image_vectors), np.array(caption_sequences)], np.array(output_sequences))\n","                image_vectors, caption_sequences, output_sequences = [], [], []\n","                n=0"],"execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## RNN model"]},{"metadata":{"trusted":true},"cell_type":"code","source":["vocab_size = len(vocab)\n","# image feature extractor model\n","inputs_image = Input(shape=(2048,))\n","fe1 = Dropout(0.5)(inputs_image)\n","fe2 = Dense(256, activation='relu')(fe1)\n","\n","# partial caption sequence model\n","inputs_caption = Input(shape=(max_length,))\n","seq1 = Embedding(vocab_size, 200)(inputs_caption)\n","seq2 = Dropout(0.5)(seq1)\n","seq3 = LSTM(256)(seq2)\n","\n","# decoder (feed forward) model\n","decoder1 = Add()([fe2, seq3])\n","decoder2 = Dense(256, activation='relu')(decoder1)\n","outputs = Dense(vocab_size, activation='softmax')(decoder2)\n","\n","# merge the two input models\n","model_RNN = Model(inputs=[inputs_image, inputs_caption], outputs=outputs)\n","model_RNN.summary()"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"functional_3\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_3 (InputLayer)            [(None, 34)]         0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            [(None, 2048)]       0                                            \n__________________________________________________________________________________________________\nembedding (Embedding)           (None, 34, 200)      330200      input_3[0][0]                    \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 2048)         0           input_2[0][0]                    \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 34, 200)      0           embedding[0][0]                  \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 256)          524544      dropout[0][0]                    \n__________________________________________________________________________________________________\nlstm (LSTM)                     (None, 256)          467968      dropout_1[0][0]                  \n__________________________________________________________________________________________________\nadd (Add)                       (None, 256)          0           dense[0][0]                      \n                                                                 lstm[0][0]                       \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 1651)         424307      dense_1[0][0]                    \n==================================================================================================\nTotal params: 1,812,811\nTrainable params: 1,812,811\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"]}]},{"metadata":{},"cell_type":"markdown","source":["## Training"]},{"metadata":{"trusted":true},"cell_type":"code","source":["model_weights = 'RNN_weights_0.0001.h5'"],"execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["model_RNN.load_weights(model_weights)\n","model_RNN.compile(loss='categorical_crossentropy', optimizer='adam')"],"execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["epochs = 10\n","batch_size_train = 16 #16 * tpu_strategy.num_replicas_in_sync\n","steps_train = len(train_descriptions)//batch_size_train"],"execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["'''\n","train_generator = data_generator(train_descriptions, w_to_ix,vocab, max_length, batch_size_train)    \n","model_RNN.fit_generator(train_generator, epochs=10, steps_per_epoch=steps_train )\n","model_RNN.save('final_model')\n","model_RNN.save_weights('model_RNN.h5')\n","'''"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ntrain_generator = data_generator(train_descriptions, w_to_ix,vocab, max_length, batch_size_train)    \\nmodel_RNN.fit_generator(train_generator, epochs=10, steps_per_epoch=steps_train )\\nmodel_RNN.save('final_model')\\nmodel_RNN.save_weights('model_RNN.h5')\\n\""]},"metadata":{},"execution_count":18}]},{"metadata":{},"cell_type":"markdown","source":["## Prediction Techniques"]},{"metadata":{},"cell_type":"markdown","source":["### Greedy Search"]},{"metadata":{"trusted":true},"cell_type":"code","source":["def greedySearch(image_id):\n","    image_feature_vector = get_image_feature_vector(image_id).reshape(1,-1)\n","    caption = 'startseq'\n","    for i in range(max_length):\n","        sequence = [w_to_ix[w] for w in caption.split(' ') if w in w_to_ix]\n","        sequence = pad_sequences([sequence], maxlen=max_length)\n","        #print(image_feature_vector.shape)\n","        predicted_vector = model_RNN.predict([image_feature_vector,sequence], verbose=0)\n","        predicted_index = np.argmax(predicted_vector)\n","        predicted_word = ix_to_w[predicted_index]\n","        caption += ' ' + predicted_word\n","        if predicted_word == 'endseq':\n","            break\n","    \n","    return caption"],"execution_count":19,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["### Beam Search"]},{"metadata":{"trusted":true},"cell_type":"code","source":["# selecting k top probablities sentences for each iteration\n","def beamSearch(image_id,k):\n","    image_feature_vector = get_image_feature_vector(image_id).reshape(1,-1)\n","    # search space is a list of size k with each caption having a probablity\n","    search_space = []\n","    caption = 'startseq'\n","    \n","    for i in range(k):\n","        search_space.append([caption,1.0])\n","        \n","    for i in range(max_length):\n","        # all_candidates store k^2 cadidate captions\n","        all_candidates=[]\n","        for j in range(k):\n","            \n","            sequence = [w_to_ix[w] for w in search_space[j][0].split(' ') if w in w_to_ix]\n","            \n","            if sequence[-1]=='endseq':\n","                all_candidates.append(search_space[j][0])\n","                continue\n","                \n","            sequence = pad_sequences([sequence], maxlen=max_length)\n","            predicted_vector = model_RNN.predict([image_feature_vector,sequence], verbose=0)\n","            #print(predicted_vector[:6])\n","            predicted_indexes = np.argsort(predicted_vector).T[-k:]\n","            #print(predicted_indexes)\n","            for index in list(predicted_indexes):\n","             #   print(index[0])\n","                predicted_word = ix_to_w[index[0]]\n","                new_caption = search_space[j][0]+ ' ' + predicted_word\n","                cur_prob = np.array(predicted_vector[:,index[0]])[0]\n","              #  print(cur_prob)\n","                caption_prob = cur_prob*search_space[j][1]\n","                all_candidates.append([new_caption,caption_prob])\n","        \n","        ordered = sorted(all_candidates, key=lambda tup:tup[1])\n","        # select k best\n","        search_space = ordered[-k:]\n","        flag=0\n","        for caption in search_space:\n","            sequence = [w for w in caption[0].split(' ')]\n","            if sequence[-1]!='endseq':\n","                flag=1\n","        if(flag==0):\n","            break\n","                \n","    return sorted(search_space, key=lambda tup:tup[1])[-1][0]"],"execution_count":20,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## Validating Model using BLEU score"]},{"metadata":{"trusted":true},"cell_type":"code","source":["def get_validation_bleu_score(greedy=True,k=1):\n","    \n","    #validation_descriptions is dict with image_id as key and list of 5 captions as value\n","    val_images_file_name = \"Flickr_Data/Flickr_TextData/Flickr_8k.devImages.txt\"\n","    val_dataset = load_data(val_images_file_name)  \n","    val_descriptions = map_captions(descriptions, val_dataset)\n","    \n","    val_predictions={} # dict with image_id as key and predicted caption as value\n","    if(greedy):  \n","        for image_id in val_descriptions.keys():\n","            caption_generated = greedySearch(image_id)\n","            val_predictions[image_id]=caption_generated\n","    else:\n","        for image_id in val_descriptions.keys():\n","            caption_generated = beamSearch(image_id,k)\n","            val_predictions[image_id]=caption_generated\n","    \n","    # candidates : list of predicted captions for each image\n","    # references : list of 5 reference captions for each image\n","    candidates , references = [],[]\n","    for image_id in val_predictions.keys():\n","        candidates.append(val_predictions[image_id])\n","        references.append(val_descriptions[image_id])\n","        \n","    val_bleu_score = corpus_bleu(references,candidates)\n","    print('val bleu score is --', val_bleu_score)\n","    \n","    bleu_score_arr = []\n","    # dict with image id as key and list of [ candidate , reference ,belu_score ]  as value, dict_i_j has images with bleu score betweeen 0.i to 0.j\n","    dict_3_4 , dict_4_5 = {} , {}\n","    for i,(candidate,reference) in enumerate(zip(candidates,references)):\n","        sent_bleu = sentence_bleu(reference,candidate)\n","        bleu_score_arr.append(sent_bleu)\n","        #image_id = validation_predictions.keys()[i]\n","        #dict_value = [candidate,reference,sent_bleu]\n","        \n","        #if(sent_bleu < 0.4):\n","         #   dict_3_4[image_id]=value\n","        #elif(sent_bleu < 0.5):\n","         #   dict_4_5[image_id]=value\n","                                            \n","    return bleu_score_arr"],"execution_count":22,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## Testing "]},{"metadata":{"trusted":true},"cell_type":"code","source":[],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def get_test_bleu_score(greedy=True,k=1):\n","    \n","    #test_descriptions is dict with image_id as key and list of 5 captions as value\n","    test_images_file_name = \"Flickr_Data/Flickr_TextData/Flickr_8k.testImages.txt\"\n","    test_dataset = load_data(test_images_file_name)  \n","    test_descriptions = map_captions(descriptions, test_dataset)\n","    \n","    test_predictions={} # dict with image_id as key and predicted caption as value\n","    \n","    if(greedy):\n","        for image_id in test_descriptions.keys():\n","            caption_generated = greedySearch(image_id)\n","            test_predictions[image_id]=caption_generated\n","    else:\n","        for image_id in test_descriptions.keys():\n","            caption_generated = beamSearch(image_id,k)\n","            test_predictions[image_id]=caption_generated\n","    \n","    # candidates : list of predicted captions for each image\n","    # references : list of 5 reference captions for each image\n","    candidates , references = [],[]\n","    for image_id in test_predictions.keys():\n","        candidates.append(test_predictions[image_id])\n","        references.append(test_descriptions[image_id])\n","        \n","    test_bleu_score = corpus_bleu(references,candidates)\n","    print('test bleu score is --', test_bleu_score)\n","    bleu_score_arr = []\n","    # dict with image id as key and list of [ candidate , reference ,belu_score ]  as value, dict_i_j has images with bleu score betweeen 0.i to 0.j\n","    dict_3_4 , dict_4_5 = {} , {}\n","    for i,(candidate,reference) in enumerate(zip(candidates,references)):\n","        sent_bleu = sentence_bleu(reference,candidate)\n","        bleu_score_arr.append(sent_bleu)\n","        image_id = list(test_predictions.keys())[i]\n","        dict_value = [candidate,reference,sent_bleu]\n","        \n","        if(sent_bleu < 0.4):\n","            dict_3_4[image_id]=dict_value\n","        elif(sent_bleu < 0.5):\n","            dict_4_5[image_id]=dict_value\n","                                            \n","                                            \n","    return bleu_score_arr , dict_3_4 , dict_4_5"],"execution_count":23,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":[]},{"metadata":{},"cell_type":"markdown","source":["# Analysis of bleu score"]},{"metadata":{"trusted":true},"cell_type":"code","source":["###histogram of test bleu score using greedy search\n","greedy_test_scores, greedy_test_dict_3_4, greedy_test_dict_4_5 = get_test_bleu_score()\n","bins = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n","plt.figure(figsize=[10,8])\n","plt.hist(greedy_test_scores,bins)\n","plt.xlabel('Test Bleu Scores')\n","plt.ylabel('Count')\n","plt.title('Bleu Scores on Test Data using Greedy Search')\n","plt.savefig('Test_Greedy.jpg')\n","plt.show()"],"execution_count":24,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-2942876ccef4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m###histogram of test bleu score using greedy search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgreedy_test_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreedy_test_dict_3_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreedy_test_dict_4_5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_test_bleu_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgreedy_test_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-23-dabfa60c0941>\u001b[0m in \u001b[0;36mget_test_bleu_score\u001b[0;34m(greedy, k)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgreedy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimage_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_descriptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mcaption_generated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgreedySearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mtest_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaption_generated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-7bb91bf98d3c>\u001b[0m in \u001b[0;36mgreedySearch\u001b[0;34m(image_id)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgreedySearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mimage_feature_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_image_feature_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcaption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'startseq'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw_to_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw_to_ix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-ec6bbda4d74c>\u001b[0m in \u001b[0;36mget_image_feature_vector\u001b[0;34m(image_id)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mimg_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mimage_feature_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_CNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_arr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Get the encoding vector for the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mimage_feature_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_feature_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_feature_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# reshape from (1, 2048) to (2048, )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimage_feature_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n","\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1593\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1596\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1139\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \"\"\"\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n","\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec, job_token)\u001b[0m\n\u001b[1;32m    694\u001b[0m           context.context().device_spec.device_type != \"CPU\"):\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/cpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    720\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[1;32m    721\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_job_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m         \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         gen_experimental_dataset_ops.make_data_service_iterator(\n","\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3003\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3004\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3005\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3006\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MakeIterator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3007\u001b[0m         tld.op_callbacks, dataset, iterator)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":["for image_id in greedy_test_dict_4_5.keys():\n","    image_folder_path = 'Flickr_Data/Images/'\n","    image_path = image_folder_path + image_id + '.jpg'\n","    img = image.load_img(image_path, target_size=(299, 299))\n","    plt.imshow(img)\n","    plt.show()\n","    print(image_id)\n","    print(greedy_test_dict_4_5[image_id][0] )\n","    print(greedy_test_dict_4_5[image_id][1][0] )\n","    print(greedy_test_dict_4_5[image_id][2] )\n","    print()"],"execution_count":25,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'greedy_test_dict_4_5' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-0ad73edace33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimage_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgreedy_test_dict_4_5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mimage_folder_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Flickr_Data/Images/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_folder_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimage_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m299\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m299\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'greedy_test_dict_4_5' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":["###histogram of validation bleu score using greedy search\n","greedy_val_scores = get_validation_bleu_score()\n","bins = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n","plt.figure(figsize=[10,8])\n","plt.hist(greedy_val_scores,bins)\n","plt.xlabel('Validation Bleu Scores')\n","plt.ylabel('Count')\n","plt.title('Bleu Scores on Validation Data using Greedy Search')\n","plt.savefig('Validation_Greedy.jpg')\n","plt.show()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["###histogram of test bleu score using beam search\n","scores = []\n","for k in range(2,5):\n","    score = get_test_bleu_score(False,k)\n","    scores.append(score)\n","bins = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n","plt.figure(figsize=[10,8])\n","plt.hist(scores,bins)\n","plt.xlabel('Test Bleu Scores')\n","plt.ylabel('Count')\n","plt.title('Bleu Scores on Test Data using Beam Search')\n","plt.legend(['k=2','k=3','k=4'])\n","plt.savefig('Test_Beam.jpg')\n","plt.show()   "],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["###histogram of validation bleu score using beam search\n","scores = []\n","for k in range(2,4):\n","    score = get_validation_bleu_score(False,k)\n","    scores.append(score)\n","bins = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n","plt.figure(figsize=[10,8])\n","plt.hist(scores,bins)\n","plt.xlabel('Validation Bleu Scores')\n","plt.ylabel('Count')\n","plt.title('Bleu Scores on Validation Data using Beam Search')\n","plt.legend(['k=2','k=3'])\n","plt.savefig('Validation_Beam.jpg')\n","plt.show() "],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["##-------------------------------------------------------FINISH--------------------------------------------------------------------------"]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.8.5-final","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}