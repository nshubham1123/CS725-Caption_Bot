{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport string\nimport cv2\nfrom PIL import Image\nfrom keras.preprocessing import image\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Embedding, Input, LSTM\nfrom tensorflow.keras.layers import Add\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input\nfrom tensorflow.keras.models import Model\nimport nltk\nfrom nltk.translate.bleu_score import sentence_bleu, corpus_bleu\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## generating tokens \nmapping image name to list of captions"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"filename = \"/kaggle/input/flickr8k/Flickr_Data/Flickr_Data/Flickr_TextData/Flickr8k.token.txt\"\nfile = open(filename,\"r\")\ndoc = file.readlines()","execution_count":1,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/Flickr_Data/Flickr_TextData/Flickr8k.token.txt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-4d26ffa4e358>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Flickr_Data/Flickr_TextData/Flickr8k.token.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Flickr_Data/Flickr_TextData/Flickr8k.token.txt'"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"descriptions ={} # a dictionary where key is image_id and value is list of 5 captions \nfor line in doc:\n    tokens = line.split(\"\\t\")\n    img_id = tokens[0].split(\".\")[0]\n    img_desc = tokens[1].strip('\\n').lower() #converting all characters into lower case\n    if img_id not in descriptions:\n        descriptions[img_id]=[]\n    descriptions[img_id].append(img_desc)\ndescriptions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data cleaning\n1. removing punctuations\n2. removing single characters\n3. converting to lower case"},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare translation table for removing punctuation\ntable = str.maketrans('', '', string.punctuation)\n\nfor img_id, captions in descriptions.items():\n    for i in range(len(captions)):\n        desc = captions[i].split()\n        # remove punctuation from each token\n        desc = [w.translate(table) for w in desc]\n        desc = [word for word in desc if len(word)>1 and word.isalpha()]\n        \n        captions[i] = \" \".join(desc)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing Train captions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# <============ Prepating list of image ids of train data ===============>\ndef load_data(filename):\n    file = open(filename,'r')\n    doc = file.readlines()\n    dataset = []\n    image_folder_path = '/kaggle/input/flickr8k/Flickr_Data/Flickr_Data/Images/'\n    \n    for line in doc:\n        if len(line)<1:\n            continue\n        image_id = line.split('.')[0]\n        image_path = image_folder_path + image_id + '.jpg'\n        if(os.path.isfile(image_path)):\n            dataset.append(image_id)\n    return dataset\n \n# <================== mapping dataset images to their captions ============>\n'''\nparameter 1: 'all captions' is 'descriptions' dictionary which we created earlier\nparameter 2 : dataset: train, dev or test\n'''\ndef map_captions(all_captions, dataset): \n    dataset_desc={}\n    for img_id in dataset:\n        captions = all_captions[img_id]\n        for i in range(len(captions)):\n            if img_id not in dataset_desc:\n                dataset_desc[img_id]=[]\n            desc =captions[i].split()\n            desc = 'startseq ' + ' '.join(desc) + ' endseq'\n            dataset_desc[img_id].append(desc)\n    return dataset_desc\n\n#train_descriptions\ntrain_images_file_name = \"/kaggle/input/flickr8k/Flickr_Data/Flickr_Data/Flickr_TextData/Flickr_8k.trainImages.txt\"\ntrain_dataset = load_data(train_images_file_name)  \ntrain_descriptions = map_captions(descriptions, train_dataset)\n\n# <================== keeping words with word count >=10 =============>\ntrain_captions=[]\n\nfor key, values in train_descriptions.items():\n    for val in values:\n        train_captions.append(val)\n        \n\n# <============ getting frequency for all words ======================>\nword_counts={}\nfor cap in train_captions:\n    for w in cap.split(' '):\n        word_counts[w] = word_counts.get(w, 0) + 1\n        \n#<===================ploting bar plot of words==========================>\nx=word_counts.keys()\ny=word_counts.values()\nplt.bar(x,y)\n#plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# <============ filtering words with freq>=10 ======================>\nvocab = [w for w in word_counts if word_counts[w] >= 10]\n\n# <=========== index to word and word to index mapping===============>  \nw_to_ix={}\nix_to_w={}\ni=0\nfor word in vocab:\n    w_to_ix[word] = i\n    ix_to_w[i]=word\n    i+=1\nprint(len(w_to_ix))\nprint(len(vocab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# description with most number of words\nmax_length = max(len(d.split()) for d in train_captions)\nmax_length","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## InceptionV3 model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the inception v3 model\nmodel = InceptionV3(weights='imagenet')\n# removing the last layer (output layer) from the inception v3\nmodel_CNN = Model(model.input, model.layers[-2].output)\nmodel_CNN.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## get image feature vector\ninput is id of image"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_image_feature_vector(image_id):\n    # path of image\n    image_folder_path = '/kaggle/input/flickr8k/Flickr_Data/Flickr_Data/Images/'\n    image_path = image_folder_path + image_id + '.jpg'\n    # converting image to required size of inception model\n    #print('image id is ',image_id)\n    img = image.load_img(image_path, target_size=(299, 299))\n    # Convert image to numpy array of 3-dimensions\n    img_arr = image.img_to_array(img)\n    #rescaling image\n    imag_arr = img_arr/255.0\n    # Add one more dimension\n    img_arr = np.expand_dims(img_arr, axis=0)\n    #print('image size is' , img_arr.shape)\n    \n    # preprocess the images using preprocess_input() from inception module\n    img_arr = preprocess_input(img_arr)\n    \n    image_feature_vector = model_CNN.predict(img_arr) # Get the encoding vector for the image\n    image_feature_vector = np.reshape(image_feature_vector, image_feature_vector.shape[1]) # reshape from (1, 2048) to (2048, )\n    return image_feature_vector\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nParameter 1: Dict with key as image_id and value as list of captions associated with image\nParameter 2: word-to-index conversion dict\nParameter 3: vocabulary\nParameter 4: max_length of a sentences\nParameter 5: batch size of data generated\n'''\n# data generator, intended to be used in a call to model.fit_generator()\ndef data_generator(train_descriptions, w_to_ix, vocab, max_length, batch_size):\n    image_vectors, caption_sequences, output_sequences = [], [], []\n    n=0\n    # loop for ever over images\n    while 1:\n        for image_id, captions_list in train_descriptions.items():\n            n+=1\n            # retrieve the image vector\n            image_feature_vector = get_image_feature_vector(image_id)\n            \n            for caption in captions_list:\n                # encode the sequence\n                caption_sequence = [w_to_ix[word] for word in caption.split(' ') if word in vocab]\n                # split one sequence into multiple X, y pairs\n                for i in range(1, len(caption_sequence)):\n                    # split into input and output pair\n                    input_sequence, output_sequence = caption_sequence[:i], caption_sequence[i]\n                    # pad input sequence\n                    input_sequence = pad_sequences([input_sequence], maxlen=max_length)[0]\n                    # encode output sequence\n                    output_sequence = to_categorical([output_sequence], num_classes=len(vocab))[0]\n                    \n                    image_vectors.append(image_feature_vector)\n                    caption_sequences.append(input_sequence)\n                    output_sequences.append(output_sequence)\n                    \n            # yield the batch data\n            if n==batch_size:\n                #print(np.array(image_vectors).shape,np.array(caption_sequences).shape,np.array(output_sequences).shape)\n                yield ([np.array(image_vectors), np.array(caption_sequences)], np.array(output_sequences))\n                image_vectors, caption_sequences, output_sequences = [], [], []\n                n=0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## RNN model"},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(vocab)\n# image feature extractor model\ninputs_image = Input(shape=(2048,))\nfe1 = Dropout(0.5)(inputs_image)\nfe2 = Dense(256, activation='relu')(fe1)\n\n# partial caption sequence model\ninputs_caption = Input(shape=(max_length,))\nseq1 = Embedding(vocab_size, 200)(inputs_caption)\nseq2 = Dropout(0.5)(seq1)\nseq3 = LSTM(256)(seq2)\n\n# decoder (feed forward) model\ndecoder1 = Add()([fe2, seq3])\ndecoder2 = Dense(256, activation='relu')(decoder1)\noutputs = Dense(vocab_size, activation='softmax')(decoder2)\n\n# merge the two input models\nmodel_RNN = Model(inputs=[inputs_image, inputs_caption], outputs=outputs)\nmodel_RNN.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_weights = '/kaggle/input/iitb-caption-weight/model_RNN_v2_0.0001.h5'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_RNN.load_weights(model_weights)\nmodel_RNN.compile(loss='categorical_crossentropy', optimizer='adam')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 10\nbatch_size_train = 16 #16 * tpu_strategy.num_replicas_in_sync\nsteps_train = len(train_descriptions)//batch_size_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ntrain_generator = data_generator(train_descriptions, w_to_ix,vocab, max_length, batch_size_train)    \nmodel_RNN.fit_generator(train_generator, epochs=10, steps_per_epoch=steps_train )\nmodel_RNN.save('final_model')\nmodel_RNN.save_weights('model_RNN.h5')\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction Techniques"},{"metadata":{},"cell_type":"markdown","source":"### Greedy Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"def greedySearch(image_id):\n    image_feature_vector = get_image_feature_vector(image_id).reshape(1,-1)\n    caption = 'startseq'\n    for i in range(max_length):\n        sequence = [w_to_ix[w] for w in caption.split(' ') if w in w_to_ix]\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        #print(image_feature_vector.shape)\n        predicted_vector = model_RNN.predict([image_feature_vector,sequence], verbose=0)\n        predicted_index = np.argmax(predicted_vector)\n        predicted_word = ix_to_w[predicted_index]\n        caption += ' ' + predicted_word\n        if predicted_word == 'endseq':\n            break\n    \n    return caption","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Beam Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"# selecting k top probablities sentences for each iteration\ndef beamSearch(image_id,k):\n    image_feature_vector = get_image_feature_vector(image_id).reshape(1,-1)\n    # search space is a list of size k with each caption having a probablity\n    search_space = []\n    caption = 'startseq'\n    \n    for i in range(k):\n        search_space.append([caption,1.0])\n        \n    for i in range(max_length):\n        # all_candidates store k^2 cadidate captions\n        all_candidates=[]\n        for j in range(k):\n            \n            sequence = [w_to_ix[w] for w in search_space[j][0].split(' ') if w in w_to_ix]\n            \n            if sequence[-1]=='endseq':\n                all_candidates.append(search_space[j][0])\n                continue\n                \n            sequence = pad_sequences([sequence], maxlen=max_length)\n            predicted_vector = model_RNN.predict([image_feature_vector,sequence], verbose=0)\n            #print(predicted_vector[:6])\n            predicted_indexes = np.argsort(predicted_vector).T[-k:]\n            #print(predicted_indexes)\n            for index in list(predicted_indexes):\n             #   print(index[0])\n                predicted_word = ix_to_w[index[0]]\n                new_caption = search_space[j][0]+ ' ' + predicted_word\n                cur_prob = np.array(predicted_vector[:,index[0]])[0]\n              #  print(cur_prob)\n                caption_prob = cur_prob*search_space[j][1]\n                all_candidates.append([new_caption,caption_prob])\n        \n        ordered = sorted(all_candidates, key=lambda tup:tup[1])\n        # select k best\n        search_space = ordered[:k]\n        flag=0\n        for caption in search_space:\n            sequence = [w for w in caption[0].split(' ')]\n            if sequence[-1]!='endseq':\n                flag=1\n        if(flag==0):\n            break\n                \n    return sorted(search_space, key=lambda tup:tup[1])[0][0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Validating Model using BLEU score"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_validation_bleu_score(greedy=True,k=1):\n    \n    #validation_descriptions is dict with image_id as key and list of 5 captions as value\n    val_images_file_name = \"/kaggle/input/flickr8k/Flickr_Data/Flickr_Data/Flickr_TextData/Flickr_8k.devImages.txt\"\n    val_dataset = load_data(val_images_file_name)  \n    val_descriptions = map_captions(descriptions, val_dataset)\n    \n    val_predictions={} # dict with image_id as key and predicted caption as value\n    if(greedy):  \n        for image_id in val_descriptions.keys():\n            caption_generated = greedySearch(image_id)\n            val_predictions[image_id]=caption_generated\n    else:\n        for image_id in val_descriptions.keys():\n            caption_generated = beamSearch(image_id,k)\n            val_predictions[image_id]=caption_generated\n    \n    # candidates : list of predicted captions for each image\n    # references : list of 5 reference captions for each image\n    candidates , references = [],[]\n    for image_id in val_predictions.keys():\n        candidates.append(val_predictions[image_id])\n        references.append(val_descriptions[image_id])\n        \n    val_bleu_score = corpus_bleu(references,candidates)\n    print('val bleu score is --', val_bleu_score)\n    \n    bleu_score_arr = []\n    # dict with image id as key and list of [ candidate , reference ,belu_score ]  as value, dict_i_j has images with bleu score betweeen 0.i to 0.j\n    dict_3_4 , dict_4_5 = {} , {}\n    for i,(candidate,reference) in enumerate(zip(candidates,references)):\n        sent_bleu = sentence_bleu(reference,candidate)\n        bleu_score_arr.append(sent_bleu)\n        #image_id = validation_predictions.keys()[i]\n        #dict_value = [candidate,reference,sent_bleu]\n        \n        #if(sent_bleu < 0.4):\n         #   dict_3_4[image_id]=value\n        #elif(sent_bleu < 0.5):\n         #   dict_4_5[image_id]=value\n                                            \n    return bleu_score_arr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Testing "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_test_bleu_score(greedy=True,k=1):\n    \n    #test_descriptions is dict with image_id as key and list of 5 captions as value\n    test_images_file_name = \"/kaggle/input/flickr8k/Flickr_Data/Flickr_Data/Flickr_TextData/Flickr_8k.testImages.txt\"\n    test_dataset = load_data(test_images_file_name)  \n    test_descriptions = map_captions(descriptions, test_dataset)\n    \n    test_predictions={} # dict with image_id as key and predicted caption as value\n    \n    if(greedy):\n        for image_id in test_descriptions.keys():\n            caption_generated = greedySearch(image_id)\n            test_predictions[image_id]=caption_generated\n    else:\n        for image_id in test_descriptions.keys():\n            caption_generated = beamSearch(image_id,k)\n            test_predictions[image_id]=caption_generated\n    \n    # candidates : list of predicted captions for each image\n    # references : list of 5 reference captions for each image\n    candidates , references = [],[]\n    for image_id in test_predictions.keys():\n        candidates.append(test_predictions[image_id])\n        references.append(test_descriptions[image_id])\n        \n    test_bleu_score = corpus_bleu(references,candidates)\n    print('test bleu score is --', test_bleu_score)\n    bleu_score_arr = []\n    # dict with image id as key and list of [ candidate , reference ,belu_score ]  as value, dict_i_j has images with bleu score betweeen 0.i to 0.j\n    dict_3_4 , dict_4_5 = {} , {}\n    for i,(candidate,reference) in enumerate(zip(candidates,references)):\n        sent_bleu = sentence_bleu(reference,candidate)\n        bleu_score_arr.append(sent_bleu)\n        image_id = list(test_predictions.keys())[i]\n        dict_value = [candidate,reference,sent_bleu]\n        \n        if(sent_bleu < 0.4):\n            dict_3_4[image_id]=dict_value\n        elif(sent_bleu < 0.5):\n            dict_4_5[image_id]=dict_value\n                                            \n                                            \n    return bleu_score_arr , dict_3_4 , dict_4_5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# Analysis of bleu score"},{"metadata":{"trusted":true},"cell_type":"code","source":"###histogram of test bleu score using greedy search\ngreedy_test_scores, greedy_test_dict_3_4, greedy_test_dict_4_5 = get_test_bleu_score()\nbins = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nplt.figure(figsize=[10,8])\nplt.hist(greedy_test_scores,bins)\nplt.xlabel('Test Bleu Scores')\nplt.ylabel('Count')\nplt.title('Bleu Scores on Test Data using Greedy Search')\nplt.savefig('Test_Greedy.jpg')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for image_id in greedy_test_dict_4_5.keys():\n    image_folder_path = '/kaggle/input/flickr8k/Flickr_Data/Flickr_Data/Images/'\n    image_path = image_folder_path + image_id + '.jpg'\n    img = image.load_img(image_path, target_size=(299, 299))\n    plt.imshow(img)\n    plt.show()\n    print(image_id)\n    print(greedy_test_dict_4_5[image_id][0] )\n    print(greedy_test_dict_4_5[image_id][1][0] )\n    print(greedy_test_dict_4_5[image_id][2] )\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###histogram of validation bleu score using greedy search\ngreedy_val_scores = get_validation_bleu_score()\nbins = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nplt.figure(figsize=[10,8])\nplt.hist(greedy_val_scores,bins)\nplt.xlabel('Validation Bleu Scores')\nplt.ylabel('Count')\nplt.title('Bleu Scores on Validation Data using Greedy Search')\nplt.savefig('Validation_Greedy.jpg')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###histogram of test bleu score using beam search\nscores = []\nfor k in range(2,5):\n    score = get_test_bleu_score(False,k)\n    scores.append(score)\nbins = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nplt.figure(figsize=[10,8])\nplt.hist(scores,bins)\nplt.xlabel('Test Bleu Scores')\nplt.ylabel('Count')\nplt.title('Bleu Scores on Test Data using Beam Search')\nplt.legend(['k=2','k=3','k=4'])\nplt.savefig('Test_Beam.jpg')\nplt.show()   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###histogram of validation bleu score using beam search\nscores = []\nfor k in range(2,4):\n    score = get_validation_bleu_score(False,k)\n    scores.append(score)\nbins = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nplt.figure(figsize=[10,8])\nplt.hist(scores,bins)\nplt.xlabel('Validation Bleu Scores')\nplt.ylabel('Count')\nplt.title('Bleu Scores on Validation Data using Beam Search')\nplt.legend(['k=2','k=3'])\nplt.savefig('Validation_Beam.jpg')\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##-------------------------------------------------------FINISH--------------------------------------------------------------------------"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}